"""
Aegis Training Script - Integration with C++ Cave Generator
Loads caves from cave_data.json generated by cavetown.exe
"""

import json
import numpy as np
import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback
from stable_baselines3.common.monitor import Monitor
import matplotlib.pyplot as plt
from datetime import datetime
import os
import subprocess
import time

class CppCaveEnvironment(gym.Env):
    """
    Environment that uses caves generated by your C++ cavetown.exe
    """
    metadata = {'render_modes': ['human', 'rgb_array']}
    
    def __init__(self, cave_generator_path="./cavetown.exe", render_mode=None):
        super().__init__()
        
        self.cave_generator_path = cave_generator_path
        self.render_mode = render_mode
        
        
        self.cave_grid = None
        self.width = 50
        self.height = 25
        
      
        self.agent_pos = None
        self.goal_pos = None
        self.steps = 0
        self.max_steps = 500
        
      
        self.action_space = spaces.Discrete(4)
        
        
        self.observation_space = spaces.Box(
            low=0, high=1, shape=(10,), dtype=np.float32
        )
        
        
        self._generate_new_cave()
    
    def _generate_new_cave(self):
        try:
            if os.path.exists(self.cave_generator_path):
                subprocess.run([self.cave_generator_path], 
                             capture_output=True, 
                             timeout=5)
                time.sleep(0.1)  
            
            if os.path.exists('cave_data.json'):
                with open('cave_data.json', 'r') as f:
                    data = json.load(f)
                    self.cave_grid = np.array(data['grid'])
                    self.width = data['width']
                    self.height = data['height']
                    return True
        except Exception as e:
            print(f"Warning: Could not generate cave with C++: {e}")
            print("Falling back to Python generation...")
        
      
        self._generate_fallback_cave()
        return False
    
    def _generate_fallback_cave(self):
        
        self.cave_grid = np.random.random((self.height, self.width)) > 0.6
        self.cave_grid = self.cave_grid.astype(int)
        
        self.cave_grid[0, :] = 1
        self.cave_grid[-1, :] = 1
        self.cave_grid[:, 0] = 1
        self.cave_grid[:, -1] = 1
    
    def _find_valid_position(self):
        open_positions = np.argwhere(self.cave_grid == 0)
        if len(open_positions) == 0:
            self._generate_new_cave()
            return self._find_valid_position()
        idx = np.random.randint(0, len(open_positions))
        return open_positions[idx]
    
    def _get_distance_sensors(self):
        directions = [
            (0, 1),   
            (1, 1),   
            (1, 0),   
            (1, -1),  
            (0, -1), 
            (-1, -1), 
            (-1, 0), 
            (-1, 1)   
        ]
        
        distances = []
        max_distance = 10
        
        for dx, dy in directions:
            distance = 0
            for step in range(1, max_distance + 1):
                check_y = self.agent_pos[0] + dy * step
                check_x = self.agent_pos[1] + dx * step
                
                if (check_y < 0 or check_y >= self.height or 
                    check_x < 0 or check_x >= self.width or
                    self.cave_grid[check_y, check_x] == 1):
                    distance = step
                    break
            else:
                distance = max_distance
            
            distances.append(distance / max_distance)
        
        return np.array(distances, dtype=np.float32)
    
    def _get_obs(self):
        sensors = self._get_distance_sensors()
        
        dy = (self.goal_pos[0] - self.agent_pos[0]) / self.height
        dx = (self.goal_pos[1] - self.agent_pos[1]) / self.width
        
        obs = np.concatenate([sensors, [dx, dy]])
        return obs.astype(np.float32)
    
    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        
        if seed is None or seed % 10 == 0:
            self._generate_new_cave()
        
        self.agent_pos = self._find_valid_position()
        
        max_attempts = 100
        for _ in range(max_attempts):
            self.goal_pos = self._find_valid_position()
            dist = np.linalg.norm(self.goal_pos - self.agent_pos)
            if dist > max(self.height, self.width) * 0.3:
                break
        
        self.steps = 0
        
        return self._get_obs(), {}
    
    def step(self, action):
        self.steps += 1
        
        old_pos = self.agent_pos.copy()
        
        if action == 0:  
            self.agent_pos[0] -= 1
        elif action == 1: 
            self.agent_pos[0] += 1
        elif action == 2: 
            self.agent_pos[1] += 1
        elif action == 3: 
            self.agent_pos[1] -= 1
        
        collision = False
        if (self.agent_pos[0] < 0 or self.agent_pos[0] >= self.height or
            self.agent_pos[1] < 0 or self.agent_pos[1] >= self.width or
            self.cave_grid[self.agent_pos[0], self.agent_pos[1]] == 1):
            collision = True
            self.agent_pos = old_pos  
        
        old_dist = np.linalg.norm(old_pos - self.goal_pos)
        new_dist = np.linalg.norm(self.agent_pos - self.goal_pos)
        
        reward = 0
        terminated = False
        
        if np.array_equal(self.agent_pos, self.goal_pos):
            reward = 100
            terminated = True
        
        elif collision:
            reward = -10
        
        else:
            reward = (old_dist - new_dist) * 1.0
        
        reward -= 0.05
        
        
        truncated = self.steps >= self.max_steps
        
        return self._get_obs(), reward, terminated, truncated, {}
    
    def render(self):
        if self.render_mode == "human":
            print("\n" + "="*self.width)
            for y in range(self.height):
                row = ""
                for x in range(self.width):
                    if [y, x] == self.agent_pos.tolist():
                        row += 'A'
                    elif [y, x] == self.goal_pos.tolist():
                        row += 'G'
                    elif self.cave_grid[y, x] == 1:
                        row += '#'
                    else:
                        row += '.'
                print(row)
            print("="*self.width)
            print(f"Steps: {self.steps}, Position: {self.agent_pos}")

def make_env(rank, cave_gen_path="./cavetown.exe", seed=0):
    
    def _init():
        env = CppCaveEnvironment(cave_generator_path=cave_gen_path)
        env = Monitor(env)
        env.reset(seed=seed + rank)
        return env
    return _init

def train(cave_generator_path="./cavetown.exe", 
          total_timesteps=1_000_000,
          num_parallel_envs=4):
    
    print("Starting Aegis Training with YOUR C++ Cave Generator!")
    print("="*60)
    
    if os.path.exists(cave_generator_path):
        print(f"Found cave generator: {cave_generator_path}")
    else:
        print(f"⚠️  Cave generator not found at: {cave_generator_path}")
        print("   Will use Python fallback generation")
    
    os.makedirs("models/checkpoints", exist_ok=True)
    os.makedirs("logs", exist_ok=True)
    
    envs = [make_env(i, cave_generator_path) for i in range(num_parallel_envs)]
    env = SubprocVecEnv(envs)
    
    eval_env = CppCaveEnvironment(cave_generator_path=cave_generator_path)
    eval_env = Monitor(eval_env)
    
    checkpoint_callback = CheckpointCallback(
        save_freq=10000 // num_parallel_envs,
        save_path='models/checkpoints/',
        name_prefix='aegis_cave'
    )
    
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path='models/',
        log_path='logs/',
        eval_freq=5000 // num_parallel_envs,
        n_eval_episodes=5,
        deterministic=True
    )
    
   
    print("\n Creating PPO model...")
    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        verbose=1,
        tensorboard_log="logs/tensorboard/"
    )
    
    
    print(f"\n Training for {total_timesteps:,} steps...")
    print("This will use caves generated by YOUR C++ code!")
    print("-" * 60)
    
    start_time = time.time()
    
    model.learn(
        total_timesteps=total_timesteps,
        callback=[checkpoint_callback, eval_callback],
        progress_bar=True
    )
    
    training_time = time.time() - start_time
    
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_path = f"models/aegis_cave_final_{timestamp}"
    model.save(model_path)
    
    print("\n" + "="*60)
    print(f" Training Complete!")
    print(f" Training time: {training_time/60:.1f} minutes")
    print(f" Model saved: {model_path}.zip")
    print("="*60)
    
    return model

def test_model(model_path, num_episodes=5, render=True):
    
    print(f"\n Testing model: {model_path}")
    
    
    model = PPO.load(model_path)
    
    
    env = CppCaveEnvironment(
        cave_generator_path="./cavetown.exe",
        render_mode="human" if render else None
    )
    
    success_count = 0
    total_rewards = []
    total_steps = []
    
    for episode in range(num_episodes):
        obs, _ = env.reset()
        done = False
        episode_reward = 0
        steps = 0
        
        print(f"\n Episode {episode + 1}/{num_episodes}")
        if render:
            env.render()
        
        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            episode_reward += reward
            steps += 1
            
            if render and steps % 50 == 0:
                env.render()
        
        if render:
            env.render()
        
        if terminated:
            success_count += 1
            print(f"SUCCESS! Reached goal in {steps} steps")
        else:
            print(f"Timeout after {steps} steps")
        
        print(f"Episode reward: {episode_reward:.2f}")
        
        total_rewards.append(episode_reward)
        total_steps.append(steps)
    
    
    print("\n" + "="*60)
    print("TESTING RESULTS:")
    print(f"   Success rate: {success_count}/{num_episodes} ({success_count/num_episodes*100:.1f}%)")
    print(f"   Average reward: {np.mean(total_rewards):.2f}")
    print(f"   Average steps: {np.mean(total_steps):.1f}")
    print("="*60)

if __name__ == "__main__":
    import sys
    
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "test":
            if len(sys.argv) < 3:
                print("Usage: python train.py test <model_path>")
                print("Example: python train.py test models/aegis_cave_final_20260131_143022.zip")
            else:
                test_model(sys.argv[2])
        elif sys.argv[1] == "quick":
            print(" Quick training mode (100k steps)")
            model = train(total_timesteps=100_000, num_parallel_envs=4)
        else:
            print("Unknown command. Use: python train.py [quick|test]")
    else:
        
        print(" Full training mode (1M steps)")
        print("Tip: Use 'python train.py quick' for faster testing")
        model = train(total_timesteps=1_000_000, num_parallel_envs=8)
        
        print("\n Training complete!")
        print("To test your model, run:")
        print("python train.py test models/aegis_cave_final_<timestamp>.zip")